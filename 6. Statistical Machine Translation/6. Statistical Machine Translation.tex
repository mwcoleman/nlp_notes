\documentclass[]{article}
\usepackage{amsmath}



\begin{document}

\section{Statistical Machine Translation}
\subsection{Background: Noisy Model}
\paragraph{The general aim is to maximise faithfulness and fluency. We use the 'Noisy Channel' model, which takes the (reverse) perspective that a message has been corrupted in some way and the task is to find the original message.}

\paragraph{If we were translating from a foreign sequence $F=f_1, f_2, .., f_n$ into an English sentence $E=e_1, e_2, .., e_n$, then:}

\begin{align}
	\hat E &= argmax_E P(E|F) \\
	&= argmax_E P(F|E)P(E)
\end{align}

\paragraph{$P(F|E)$ is the translation model (corresponding to faithfulness) and requires a parallel corpus to learn probabilities. $P(E)$ is our language model (corresponding to fluency) and needs only a single language corpus (i.e. an n-gram language model).}



\subsection{Naive Translation Model (Phrase Based)}
\paragraph{Idea: Use a phrase (sequence of words) and/or single words as the fundamental units. Assuming that the parallel corpus is phrase aligned and may feature one-to-many alignments, the steps are:} 
	
\begin{enumerate}
	\item Group English words into phrases $e_1, e_2, .., e_I$ and $f_1, f_2, .., f_i$ 
	\item Translate each $e_i$ into $f_i$
	\item (optional) Reorder
\end{enumerate}

\paragraph{The overall translation probability becomes:}

\begin{align}
	P(F|E) = \prod^I \phi(f_j, e_i)d(a_i - b_{i-1})
\end{align}
$\phi(f_i, e_i)$ is the translation probability - learnt from the corpus counts \\ \\
$d(a_i - b_i) = \alpha^{|a_i - b_i|}$  is the distortion probability, a weighting to penalise larger offsets. $a_i$ is foreign word start position for phrase $i$, $b_i$ is the foreign word finish position e.g. if the foreign phrase starts at 3 and finishes at 5, then $d(a_i - b_i) = \alpha^{|3 - 5|}$
\newpage
\subsection{Word Alignment Translation Model: IBM Model 1}
\subsubsection{Definition}
\paragraph{As direct phrase alignments are difficult, the problem might get broken down into one of word alignments.}
\begin{align}
	P(F|E) = \sum^A P(F,A|E)
\end{align}
\paragraph{Let us assume the following:}	
	\begin{itemize}
		\item One to many translations possible (e.g. $I^J$ possible alignments)
		\item Words may be dropped from the source sequence
		\item Words may be generated from NULL in the source sequence: ($(I+1)^J$ alignments now
	\end{itemize}
\paragraph{Then the generative steps of producing F from E are:}
\begin{enumerate}
	\item Choose a Foreign sentence length $F=f_1, f_2, .., f_J$
	\item Choose an English sentence length $E=e_1, e_2, .., e_I$
	\item Choose word alignments for the foreign sentence $A=a_1, a_2, .., a_J$
	\item For each position in F, generate a word $f_j$ from the aligned word in $E$ ($e_{aj}$) with probability $t(f_j|e_{aj})$
\end{enumerate}
\subsubsection{Decoding: Computing $P(F|E)$ and the most probable alignment}
\paragraph{Using the word alignment model, the probability of generating F from E is the probability of F from E with some alignment.}	
\begin{align}
P(F,A|E) &= P(F|E,A) P(A|E) \\
P(F,A|E) &= \big( \prod^J t(f_j|e_{aj}) \big) \cdot \big( \frac{\epsilon}{(I+1)^J} \big)
\end{align}
\paragraph{The alignment term (second term above) is assumed uniform for all alignments and comes about from knowing there are $(I+1)^J$ possible alignments to choose from where $J$ (the length of $F$) is chosen with some small (uniform)  probability represented by $\epsilon$}
\paragraph{$A$ is our decoding (and is the latent variable). We find it by:}
\begin{align}
\hat A = argmax_A \prod^J t(f_j|e_{aj})
\end{align}
\newpage
\subsubsection{Training the model with EM: An example}
\paragraph{The parameters to be learned are $t(f_i|e_{aj})$}
\paragraph{Two sentence pairs:\\}
("green house", "casa verde") and ("the house", "la casa")
\paragraph{The vocabularies are:\\}
$V_E$:("the", "green", "house") and $V_F$:("la", "casa", "verde")
\begin{itemize}
	\item initialising $t$ with uniform probabilities:
	\begin{align}
	t(casa|green)&=\frac{1}{3} \notag \\
	t(casa|house)&=\frac{1}{3} \notag \\
	t(casa|the)&=\frac{1}{3} \notag \\
	t(verde|green)&=\frac{1}{3} \notag \\
	... \notag
	\end{align}

	\item E: Compute $P(A,F|E) = \prod^J t(f_i|e_{aj})$ for all possible sentences and their alignments (e.g. 2 sentences consisting of 2 words each equates to 4 alignments total). Normalise the results (sum over $A$)
	\item E: Get totals by adding up any duplicate pairs (e.g. "casa" and "house" are in both sentences so the counts add)
	\item M: For each English word $e_i$ normalise $t(f_j|e_i)$ 
	
\end{itemize}

\end{document}
